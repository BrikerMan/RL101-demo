{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd662ec8-a5e1-4462-8c99-40897b15890b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt-get install xvfb -y\n",
    "!pip install pyvirtualdisplay\n",
    "!pip install Pillow\n",
    "!pip install \"gym[box2d]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b681b-18c1-4dc8-ab87-650e84d96997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd311e1-bd48-4394-abf6-591d8001de1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d409800-2459-46ee-8dc5-b0db86970f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, env_name: str):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_count = self.env.action_space.n\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        return np.random.randint(self.action_count)\n",
    "\n",
    "    def simulate(self, visualize=True):\n",
    "        records = []\n",
    "        state = self.env.reset()\n",
    "        is_done = False\n",
    "        total_score = 0\n",
    "\n",
    "        while not is_done:\n",
    "            action = self.choose_action(state)\n",
    "            # 执行动作\n",
    "            observation, reward, is_done, info = self.env.step(action)\n",
    "            # 记录总分\n",
    "            total_score += reward\n",
    "            \n",
    "            # 渲染画面，得到画面的像素数组\n",
    "            rgb_array = self.env.render(mode='rgb_array')\n",
    "            # 记录中间过程，用于后期可视化\n",
    "            records.append((rgb_array, action, reward, total_score))\n",
    "\n",
    "            if visualize:\n",
    "                # 清除当前 Cell 的输出\n",
    "                display.clear_output(wait=True)\n",
    "                                \n",
    "                # 使用像素数组生成图片\n",
    "                img = Image.fromarray(rgb_array)\n",
    "                # 当前 Cell 中展示图片\n",
    "                display.display(img)\n",
    "                print(f'Action {action} Action reward {reward:.2f} | Total score {total_score:.2f}')\n",
    "                # 防止刷新过快，可以根据实际情况调小\n",
    "                time.sleep(0.01)\n",
    "        self.env.close()\n",
    "        return total_score, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3b8d2-39ef-4e08-a145-7425d6a14e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = RandomAgent('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096f4e7-ad9c-4466-9d92-d90e84d5d0d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_score, records = agent.simulate(visualize=True)\n",
    "print(f'Total score {total_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b872a8b-d266-4862-ac65-c88a6cf5c4b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "record_list = []\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "    total_score, _ = agent.simulate(visualize=False)\n",
    "    record_list.append(total_score)\n",
    "    \n",
    "print(f'Average score in 100 episode {np.mean(record_list):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c35b300-b206-4cd1-b940-5c22f2864fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "# 代表每一个样本的 namedtuple，方便存储和读取数据\n",
    "Experience = namedtuple('Experience', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.memory = []\n",
    "\n",
    "    def append(self, state, action, reward, next_state, done):\n",
    "        \"\"\"记录一个新的样本\"\"\"\n",
    "        sample = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(sample)\n",
    "        # 只留下最新记录的 self.max_size 个样本\n",
    "        self.memory = self.memory[-self.max_size:]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"按照给定批次大小取样\"\"\"\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        batch = Experience(*zip(*samples))\n",
    "\n",
    "        # 转换数据为 numpy 张量返回\n",
    "        states = np.array(batch.state)\n",
    "        actions = np.array(batch.action)\n",
    "        rewards = np.array(batch.reward)\n",
    "        states_next = np.array(batch.next_state)\n",
    "        dones = np.array(batch.done)\n",
    "\n",
    "        return states, actions, rewards, states_next, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f675b311-b87f-4cf5-9ab7-0762c983440d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "L = tf.keras.layers\n",
    "\n",
    "\n",
    "def create_network_model(input_shape: np.ndarray,\n",
    "                         action_space: np.ndarray,\n",
    "                         learning_rate=0.001) -> tf.keras.Sequential:\n",
    "    model = tf.keras.Sequential([\n",
    "        L.Dense(512, input_shape=input_shape, activation=\"relu\"),\n",
    "        L.Dense(256, input_shape=input_shape, activation=\"relu\"),\n",
    "        L.Dense(action_space)\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", \n",
    "                  optimizer=tf.optimizers.Adam(lr=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997d67c-7b79-4d67-a6d1-e7a008093dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from PIL import Image\n",
    "\n",
    "# 定义超参数\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.99\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_MIN = 0.01\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, env_name):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.observation_shape = self.env.observation_space.shape\n",
    "        self.action_count = self.env.action_space.n\n",
    "        self.model = create_network_model(self.observation_shape, self.action_count)\n",
    "        self.memory = ReplayMemory(500000)\n",
    "        self.epsilon = 1.0\n",
    "        self.batch_size = 64\n",
    "\n",
    "    def choose_action(self, state, epsilon=None):\n",
    "        \"\"\"\n",
    "        根据给定状态选择行为\n",
    "        - epsilon == 0 完全使用模型选择行为\n",
    "        - epsilon == 1 完全随机选择行为\n",
    "        \"\"\"\n",
    "        if epsilon is None:\n",
    "            epsilon = self.epsilon\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(self.action_count)\n",
    "        else:\n",
    "            q_values = self.model.predict(np.expand_dims(state, axis=0))\n",
    "            return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"进行经验回放学习\"\"\"\n",
    "\n",
    "        # 如果当前经验池经验数量少于批次大小，则跳过\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, states_next, dones = self.memory.sample(self.batch_size)\n",
    "        q_pred = self.model.predict(states)\n",
    "\n",
    "        q_next = self.model.predict(states_next).max(axis=1)\n",
    "        q_next = q_next * (1 - dones)\n",
    "        q_update = rewards + GAMMA * q_next\n",
    "\n",
    "        indices = np.arange(self.batch_size)\n",
    "        q_pred[[indices], [actions]] = q_update\n",
    "\n",
    "        self.model.train_on_batch(states, q_pred)\n",
    "\n",
    "    def simulate(self, epsilon=None, visualize=True):\n",
    "        records = []\n",
    "        state = self.env.reset()\n",
    "        is_done = False\n",
    "        total_score = 0\n",
    "        total_step  = 0\n",
    "        while not is_done:\n",
    "            action = self.choose_action(state, epsilon)\n",
    "            state, reward, is_done, info = self.env.step(action)\n",
    "            total_score += reward\n",
    "            total_step += 1\n",
    "            \n",
    "            rgb_array = self.env.render(mode='rgb_array')\n",
    "            records.append((rgb_array, action, reward, total_score))\n",
    "\n",
    "            if visualize:\n",
    "                display.clear_output(wait=True)\n",
    "                img = Image.fromarray(rgb_array)\n",
    "                # 当前 Cell 中展示图片\n",
    "                display.display(img)\n",
    "                print(f'Action {action} Action reward {reward:.2f} | Total score {total_score:.2f} | Step {total_step}')\n",
    "\n",
    "                time.sleep(0.01)\n",
    "        self.env.close()\n",
    "        return total_score, records\n",
    "\n",
    "    def train(self, episode_count: int, log_dir: str):\n",
    "        \"\"\"\n",
    "        训练方法，按照给定 episode 数量进行训练，并记录训练过程关键参数到 TensorBoard\n",
    "        \"\"\"\n",
    "        # 初始化一个 TensorBoard 记录器\n",
    "        file_writer = tf.summary.create_file_writer(log_dir)\n",
    "        file_writer.set_as_default()\n",
    "\n",
    "        score_list = []\n",
    "        best_avg_score = -np.inf\n",
    "\n",
    "        for episode_index in range(episode_count):\n",
    "            state = self.env.reset()\n",
    "            score, step = 0, 0\n",
    "            is_done = False\n",
    "            while not is_done:\n",
    "                # 根据状态选择一个行为\n",
    "                action = self.choose_action(state)\n",
    "                # 执行行为，记录行为和结果到经验池\n",
    "                state_next, reward, is_done, info = self.env.step(action)\n",
    "                self.memory.append(state, action, reward, state_next, is_done)\n",
    "                score += reward\n",
    "\n",
    "                state = state_next\n",
    "                # 每 6 步进行一次回放训练\n",
    "                # 此处也可以选择每一步回放训练，但会降低训练速度，这个是一个经验技巧\n",
    "                if step % 1 == 0:\n",
    "                    self.replay()\n",
    "                step += 1\n",
    "\n",
    "            # 记录当前 Episode 的得分，计算最后 100 Episode 的平均得分\n",
    "            score_list.append(score)\n",
    "            avg_score = np.mean(score_list[-100:])\n",
    "\n",
    "            # 记录当前 Episode 得分，epsilon 和最后 100 Episode 的平均得分到 TensorBoard\n",
    "            tf.summary.scalar('score', data=score, step=episode_index)\n",
    "            tf.summary.scalar('average score', data=avg_score, step=episode_index)\n",
    "            tf.summary.scalar('epsilon', data=self.epsilon, step=episode_index)\n",
    "\n",
    "            # 终端输出训练进度\n",
    "            print(f'Episode: {episode_index} Reward: {score:03.2f} '\n",
    "                  f'Average Reward: {avg_score:03.2f} Epsilon: {self.epsilon:.3f}')\n",
    "\n",
    "            # 调整 epsilon 值，逐渐减少随机探索比例\n",
    "            if self.epsilon > EPSILON_MIN:\n",
    "                self.epsilon *= EPSILON_DECAY\n",
    "\n",
    "            # 如果当前平均得分比之前有改善，保存模型\n",
    "            # 确保提前创建目录 outputs/chapter_15\n",
    "            if avg_score > best_avg_score:\n",
    "                best_avg_score = avg_score\n",
    "                self.model.save(f'outputs/chapter_15/dqn_best_{episode_index}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928d0b6-6faf-419a-ba51-3e171c303fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用 LunarLander 初始化 Agent\n",
    "agent = DQNAgent('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d272b6b-e103-44b5-8ac4-cfe54d698d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "# 读取现在已经记录的日志数量，避免日志重复记录\n",
    "tf_log_index = len(glob.glob('tf_dir/lunar_lander/run_*'))\n",
    "log_dir = f'tf_dir/lunar_lander/run_{tf_log_index}'\n",
    "\n",
    "# 训练 2000 个 Episode\n",
    "agent.train(2000, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fea7b6a3-3256-4e53-8395-e8d610fa8c25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T12:50:45.323058Z",
     "iopub.status.busy": "2021-10-14T12:50:45.322458Z",
     "iopub.status.idle": "2021-10-14T12:50:45.341598Z",
     "shell.execute_reply": "2021-10-14T12:50:45.340306Z",
     "shell.execute_reply.started": "2021-10-14T12:50:45.322994Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agent.model.load_weights('dqn_best.h5')\n",
    "agent.model.load_weights('dqn_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dece13c1-a476-4026-89b3-87ead19d99e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-14T12:52:15.493416Z",
     "iopub.status.busy": "2021-10-14T12:52:15.492797Z",
     "iopub.status.idle": "2021-10-14T12:52:50.728815Z",
     "shell.execute_reply": "2021-10-14T12:52:50.727404Z",
     "shell.execute_reply.started": "2021-10-14T12:52:15.493346Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x7F08E419FCC0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action 0 Action reward 100.00 | Total score 245.31 | Step 341\n"
     ]
    }
   ],
   "source": [
    "score, records = agent.simulate(epsilon=0.0, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b33c8c-f483-4a74-914a-f4a7fda5660f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp outputs/chapter_15/dqn_best.h5 dqn_best.h5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b06907-2496-4c13-9ca0-d0acbc6fec25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
